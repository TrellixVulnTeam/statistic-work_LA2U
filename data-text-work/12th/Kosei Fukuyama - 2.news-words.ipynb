{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcd2ec3-39f6-4ce8-b406-e3a2beeb8eea",
   "metadata": {},
   "source": [
    "# 一橋大学のニュースをオブジェクト保存\n",
    "\n",
    "ここまで何度も行ってきた、一橋大学のニューステキストを辞書型(dict型)に変換する作業を、Pythonのオブジェクトとして保存する。\n",
    "保存したものを、復元して使えるようにします。\n",
    "\n",
    "標準ライブラリ　``pickle`` を使うと、Pythonのオブジェクトをそのままファイルに保存することができます。\n",
    "機械学習の学習済みモデルのように生成するのに時間がかかったり、何度も使うことが有るものに使うことができます。\n",
    "\n",
    "基本的なPythonのデータ型であればpickleして保存することができます。（一部できないものもあります）\n",
    "\n",
    "このような考え方にはJSON形式にして保存する方法もありますが、データ型が維持できない点で、機械学習の分野では、pickleが一般的に使われています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19305cb9-20e6-400d-b5fe-4ac6c283eae6",
   "metadata": {},
   "source": [
    "## 辞書の生成\n",
    "\n",
    "今までどおり、ニュースのテキストから、形態素解析を行い、辞書型のオブジェクトを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8409b4c-af0e-4b42-aa2d-ced41326134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import base64\n",
    "\n",
    "out = []\n",
    "text = \"\"\n",
    "\n",
    "url = \"https://hit-u-data-text-processing.herokuapp.com/data/20210502-news-text.txt\"\n",
    "auth_str = base64.b64encode(b\"reader:hit-u\")\n",
    "req = urllib.request.Request(url,\n",
    "                            headers={\"Authorization\": \"Basic \" + auth_str.decode(\"utf-8\")})\n",
    "with urllib.request.urlopen(req) as req:\n",
    "    text = req.read().decode(\"utf-8\")\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if line.strip() == \"===\":\n",
    "            if text:\n",
    "                out.append(text)\n",
    "            text = \"\"\n",
    "        else:\n",
    "            text += line\n",
    "url = \"https://hit-u-data-text-processing.herokuapp.com/data/20210612-news-text.txt\"\n",
    "auth_str = base64.b64encode(b\"reader:hit-u\")\n",
    "req = urllib.request.Request(url,\n",
    "                            headers={\"Authorization\": \"Basic \" + auth_str.decode(\"utf-8\")})\n",
    "with urllib.request.urlopen(req) as req:\n",
    "    text = req.read().decode(\"utf-8\")\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if line.strip() == \"===\":\n",
    "            if text:\n",
    "                out.append(text)\n",
    "            text = \"\"\n",
    "        else:\n",
    "            text += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51f1f17-a4f7-408e-a714-b80489774ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d0bcd6-af31-4952-8eca-baa9b63f89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from janome.tokenfilter import TokenFilter\n",
    "from janome.tokenfilter import CompoundNounFilter\n",
    "from janome.tokenfilter import POSKeepFilter\n",
    "from janome.tokenfilter import LowerCaseFilter\n",
    "from janome.charfilter import UnicodeNormalizeCharFilter\n",
    "from janome.analyzer import Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a15f08b-689d-49b6-987f-aa501ac05bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWordFilter(TokenFilter):\n",
    "    def __init__(self, words):\n",
    "        self.stop_words = words\n",
    "    \n",
    "    def apply(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token.surface not in self.stop_words:\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908a1060-cf7f-4101-babb-94619471d5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['大学', '一橋大学', '===', 'こと', 'the', 'ため', 'よう', 'of', '(', ')', '様']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import base64\n",
    "url = \"https://hit-u-data-text-processing.herokuapp.com/data/stopwords.txt\"\n",
    "auth_str = base64.b64encode(b\"reader:hit-u\")\n",
    "req = urllib.request.Request(url,\n",
    "                            headers={\"Authorization\": \"Basic \" + auth_str.decode(\"utf-8\")})\n",
    "\n",
    "stop_words = []\n",
    "with urllib.request.urlopen(req) as req:\n",
    "    lines = req.read().decode(\"utf-8\")\n",
    "    for line in lines.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            stop_words.append(line.strip())\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd507fe0-2a72-41dc-92df-08244c222271",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_filter = StopWordFilter(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69065ab-8da5-402b-a48a-3db919f2386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_filters = [CompoundNounFilter(),\n",
    "                POSKeepFilter([\"名詞\", \"動詞\", \"形容詞\"]),\n",
    "                LowerCaseFilter(),\n",
    "                stop_word_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009dd325-7312-42fb-b931-1b9f93f9dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_filters = [UnicodeNormalizeCharFilter()]\n",
    "tokenizer = Tokenizer()\n",
    "analyzer = Analyzer(char_filters=char_filters, \n",
    "                    tokenizer=tokenizer, \n",
    "                    token_filters=token_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ff5bb3-fbc5-4477-8019-ed8ee734468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_words = {}\n",
    "for i, text in enumerate(out):\n",
    "    docs_words[i] = []\n",
    "    for token in analyzer.analyze(text):\n",
    "        docs_words[i].append(token.base_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40247e94-452f-424c-8537-00299b53518a",
   "metadata": {},
   "source": [
    "## pickleで永続化\n",
    "\n",
    "Pythonのオブジェクトをシリアライズ化するpickeを使って、ファイルに保存（永続化)を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9cba847-22b5-4213-ac77-1078dc706993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65c73324-78dc-40cb-9036-8340ce0a3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docs_words.picke\", \"wb\") as f:\n",
    "    pickle.dump(docs_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629d893-9186-4c4c-ad68-1383846a90b2",
   "metadata": {},
   "source": [
    "docs_words.picke というファイルが出来上がりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978403d-fea4-45e5-862a-633348a5c505",
   "metadata": {},
   "source": [
    "## pickeデータの復元\n",
    "\n",
    "データを復元してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065b3e55-9be1-41d0-bec8-540ab9f94650",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docs_words.picke\", \"rb\") as f:\n",
    "    docs_words_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e86b045-7849-48a0-8a42-f0a21b73388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs_words_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63c671c-1dbc-4919-b146-c9168a246f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ソニー',\n",
       " 'パナソニック',\n",
       " '富士通',\n",
       " '資生堂',\n",
       " '共同',\n",
       " 'デザイン組織',\n",
       " '共通評価指標',\n",
       " '検討',\n",
       " '作成',\n",
       " '2021年4月30日']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_words_new[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dc27631-1871-4b94-b66c-96c5f1f3c36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_words is docs_words_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e4bf530-e640-4ff8-b4d1-8bd016315669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_words == docs_words_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
